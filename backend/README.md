# Backend â€“ AI Assistant API

This folder contains the **FastAPI backend** responsible for:
- receiving page context from the extension
- generating proactive suggestions
- answering user questions using an AI agent
- enforcing strict context grounding

---

## Core Design Principles

- **Stateless API** (Cloud Run friendly)
- **Context-bounded reasoning** (anti-hallucination)
- **Deterministic fallbacks**
- **LLM observability**

---

## ðŸ—‚ Folder Structure

backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ app.py # FastAPI routes
â”‚ â”œâ”€â”€ agent/
â”‚ â”‚ â””â”€â”€ graph.py # LangGraph agent definition
â”‚ â”œâ”€â”€ llm.py # LLM instantiation (Vertex AI)
â”‚ â”œâ”€â”€ extractors.py # Context compaction & JSON parsing
â”‚ â”œâ”€â”€ suggestions.py # Rule-based suggestions
â”‚ â”œâ”€â”€ schemas.py # Pydantic models
â”‚ â””â”€â”€ config.py # Environment-based configuration
â”œâ”€â”€ main.py # Uvicorn entrypoint
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## Agent Architecture (LangGraph)

The AI agent is implemented as a **state graph**, not a linear chain.

### Agent States

```python
AgentState = {
  messages: conversation history,
  context: page context,
  suggestions: proactive prompts,
  final_response: assistant answer
}
```

### Routing Logic

- If no user message â†’ proactive_analysis
- If user message exists â†’ chatbot

This clean separation allows independent evolution of proactive and reactive behaviors.

### Anti-Hallucination Strategy

- Page context injected into system prompt
- Explicit instruction to not infer missing information
- No external browsing
- No persistent memory

This ensures responses remain grounded in visible data only.

### Observability

Langfuse is used to trace:

- LLM calls
- latency per node
- token usage
- errors and fallbacks

### Running locally

```bash
cd backend
python main.py
```
